# Data-Pipeline_Retail

We create a data pipeline to analyse sales at a retail stores during the holiday season. We extract and merge data from two fictional databases (`grocery_sales` from a PostgreSQL database and complementary data from a `extra_data.parquet` file). We then transform and analyse the data before loading the transformed dataframe and analysis onto a specified path. Finally, we validate that the files were successfully saved into the directory. 

# `grocery_sales`
- `"index"` - unique ID of the row
- `"Store_ID"` - the store number
- `"Date"` - the week of sales
- `"Weekly_Sales"` - sales for the given store

- # `extra_data.parquet`
- `"IsHoliday"` - Whether the week contains a public holiday - 1 if yes, 0 if no.
- `"Temperature"` - Temperature on the day of sale
- `"Fuel_Price"` - Cost of fuel in the region
- `"CPI"` â€“ Prevailing consumer price index
- `"Unemployment"` - The prevailing unemployment rate
- `"MarkDown1"`, `"MarkDown2"`, `"MarkDown3"`, `"MarkDown4"` - number of promotional markdowns
- `"Dept"` - Department Number in each store
- `"Size"` - size of the store
- `"Type"` - type of the store (depends on `Size` column)
